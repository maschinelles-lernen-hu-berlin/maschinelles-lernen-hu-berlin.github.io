<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>2 | Lehrstuhl Maschinelles Lernen</title><link>https://example.com/publications-type/2/</link><atom:link href="https://example.com/publications-type/2/index.xml" rel="self" type="application/rss+xml"/><description>2</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Wed, 01 Aug 2018 00:00:00 +0000</lastBuildDate><image><url>https://example.com/media/logo_hueebd31b28f82aeb63357b594dab7cdfb_24654_300x300_fit_lanczos_3.png</url><title>2</title><link>https://example.com/publications-type/2/</link></image><item><title>Contextual String Embeddings for Sequence Labeling</title><link>https://example.com/publications/2018-contextual-string-embeddings/</link><pubDate>Wed, 01 Aug 2018 00:00:00 +0000</pubDate><guid>https://example.com/publications/2018-contextual-string-embeddings/</guid><description>&lt;p>Recent advances in language modeling using recurrent neural networks have made it viable to model language as distributions over characters. By learning to predict the next character on the basis of previous characters, such models have been shown to automatically internalize linguistic concepts such as words, sentences, subclauses and even sentiment. In this paper, we propose to leverage the internal states of a trained character language model to produce a novel type of word embedding which we refer to as contextual string embeddings. Our proposed embeddings have the distinct properties that they (a) are trained without any explicit notion of words and thus fundamentally model words as sequences of characters, and (b) are contextualized by their surrounding text, meaning that the same word will have different embeddings depending on its contextual use. We conduct a comparative evaluation against previous embeddings and find that our embeddings are highly useful for downstream tasks: across four classic sequence labeling tasks we consistently outperform the previous state-of-the-art. In particular, we significantly outperform previous work on English and German named entity recognition (NER), allowing us to report new state-of-the-art F1-scores on the CoNLL03 shared task. We release all code and pre-trained language models in a simple-to-use framework to the research community, to enable reproduction of these experiments and application of our proposed embeddings to other tasks: &lt;a href="https://github.com/zalandoresearch/flair" target="_blank" rel="noopener">https://github.com/zalandoresearch/flair&lt;/a>&lt;/p></description></item></channel></rss>